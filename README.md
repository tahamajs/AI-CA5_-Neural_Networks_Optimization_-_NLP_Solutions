# AI-CA5 & HW5: Neural Networks, Optimization & NLP Solutions

## Introduction

This repository contains solutions for **Artificial Intelligence - CA5 and HW5** assignments. These assignments cover essential AI concepts such as  **Neural Networks, Backpropagation, Convolutional Neural Networks (CNNs), Feature Selection, Optimization, and Natural Language Processing (NLP)** . The project is designed to provide an in-depth understanding of machine learning techniques, with a focus on improving neural network performance through optimization and regularization methods.

---

## Key Topics Covered

### 1. Neural Networks and Backpropagation

* Implementation of feedforward and backpropagation algorithms
* Understanding weight updates using **Gradient Descent**
* Calculation of **cost function** to minimize errors
* Application of **activation functions** such as **ReLU, Sigmoid, and Tanh**
* Effect of different **learning rates** on training efficiency

### 2. Convolutional Neural Networks (CNNs)

* Use of convolution layers for **feature extraction**
* Importance of **pooling layers (MaxPooling, AveragePooling)**
* Implementing CNN-based image classification models
* Evaluating performance with **accuracy and loss metrics**

### 3. Optimization and Regularization Techniques

* **Gradient Descent Variants** : Stochastic, Mini-batch, Adam, RMSProp
* **Regularization Methods** to prevent overfitting:
  * **Dropout** (randomly ignoring neurons to enhance generalization)
  * **Batch Normalization** (improving stability in training)

### 4. Natural Language Processing (NLP)

* **Word Embeddings** : How words are represented in vector space
* Implementation of **Word2Vec & GloVe** for semantic similarity
* Understanding **text vectorization** for machine learning applications

---

---

## Key Questions Addressed

### Neural Networks

* How does backpropagation adjust weights in a neural network?
* What is the role of the **cost function** in training deep models?
* How do different activation functions affect model performance?
* How can neural networks be **regularized** to avoid overfitting?

### Convolutional Neural Networks (CNNs)

* How do CNNs extract important features for  **image classification** ?
* What are the effects of **different pooling methods** on model performance?
* Why do CNNs outperform traditional machine learning models on image data?

### Optimization and Learning Efficiency

* How does **learning rate scheduling** impact convergence?
* What are the differences between **SGD, Adam, and RMSProp** optimizers?

### Natural Language Processing (NLP)

* What is  **Word2Vec** , and how does it represent text data in vector space?
* How do word embeddings help in  **text classification and sentiment analysis** ?

---

## Results and Performance Evaluation

The solutions provided in this repository include model training results, which are evaluated based on:

* **Training vs Validation Accuracy** over epochs
* **Effectiveness of Regularization Techniques** (Dropout, Batch Normalization)
* **Performance Comparison of Activation Functions**
* **Optimization Methods Impact on Convergence Speed**
* **Feature Selection and Dimensionality Reduction Analysis**

Results and analysis are stored in the `results/` folder.

---
